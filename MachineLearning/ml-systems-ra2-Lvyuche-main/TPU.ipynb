{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uscmlsystems/ml-systems-ra2-Lvyuche/blob/main/TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA0rETRQZBON"
      },
      "source": [
        "# EE 599 Reading Assignment 2:\n",
        "You can access paper using this [link](https://arxiv.org/pdf/1704.04760.pdf). Please refer to this [guideline](http://ccr.sigcomm.org/online/files/p83-keshavA.pdf) on how to read a research paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-HWWv6kaxrx"
      },
      "source": [
        "### Q1: (1 Point)\n",
        "\n",
        "**Disallowed** list:\n",
        "- You **MAY NOT** collaborate with anyone else on this assignment. This means you cannot talk to anyone else about the assignment until after deadline.\n",
        "- You **MAY NOT** use ChatGPT and services like that\n",
        "\n",
        "**Allowed** list:\n",
        "- Notes including any slides from the class\n",
        "- The textbooks\n",
        "- The given paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz0uD-nHbHuT"
      },
      "source": [
        "### A1:\n",
        "\n",
        "I affirm I have read these exam rules and will follow them. Failure to do so may subject me to sanctions including an F in the course.\n",
        "\n",
        "**Type your full name to affirm you have read the above statement:**\n",
        "Yuxuan Lyu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzNWLVCLbcjq"
      },
      "source": [
        "----\n",
        "### Q2 Summary (24 Points):\n",
        "\n",
        "Summarize the main objectives and contributions of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j5A4_Zfbpra"
      },
      "source": [
        "### A2:\n",
        "The paper aims to evaluate Google's Tensor Processing Unit (TPU) in terms of performance, power efficiency, and its ability to accelerate neural network inference in datacenters, comparing it with contemporary CPUs and GPUs. It highlights the TPU's superior energy efficiency and faster inference speed, underscoring its deterministic execution model suited for low-latency, user-facing applications. A key insight from the study reveals that convolutional neural networks (CNNs) constitute only a small fraction of datacenter workloads, suggesting a broader application scope for TPUs across various neural network architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE2n5UQHbuZ-"
      },
      "source": [
        "---\n",
        "### Q3 Comprehension (15 Points):\n",
        "- What is a Tensor Processing Unit (TPU) and how does it differ from traditional CPUs and GPUs in terms of design and purpose?\n",
        "- Why is there a need for specialized hardware like TPUs in modern data centers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O7k2DbXcNp0"
      },
      "source": [
        "### A3:\n",
        "1. A Tensor Processing Unit (TPU) is a specialized chip optimized for neural network operations, offering faster and more energy-efficient processing than CPUs and GPUs. TPUs are designed for quick neural network inference, using a simple, single-threaded approach for rapid and predictable task execution, making them highly suitable for applications requiring fast responses.  \n",
        "2. Modern data centers utilize specialized hardware like TPUs for efficient management of deep neural network (DNN) applications. TPUs enhance performance and lower costs by accelerating computations and optimizing energy use, outperforming general-purpose CPUs and GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnzcxvhWckY5"
      },
      "source": [
        "---\n",
        "### Q4 Technical Deep Dive (15 Points):\n",
        "- Describe the architecture of the TPU. How is it optimized for machine learning workloads?\n",
        "- How does the memory hierarchy of the TPU differ from traditional processors, and why is this significant for tensor computations?\n",
        "- What are systolic arrays, and why are they used in TPUs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv3rFNjpcuID"
      },
      "source": [
        "### A4:\n",
        "1. The TPU architecture has a Matrix Multiply Unit with 256x256 MACs for efficient 8-bit calculations prevalent in machine learning. It boasts substantial on-chip memory for immediate data access, an Activation Unit for non-linear functions crucial to neural networks, and employs a Weight FIFO and Unified Buffer for effective data flow management, guaranteeing high performance and swift response times in machine learning operations.   \n",
        "2. The TPU's Matrix Multiply Unit employs a systolic array, unlike traditional processors, significantly lowering power use by reducing read and write operations to large SRAMs.  \n",
        "3. Systolic arrays are a network of processors that rhythmically pass data and instructions to each other, much like the human heart pumps blood. They are used in TPUs to efficiently perform matrix operations vital for machine learning tasks. This design allows for high-speed data processing with lower power consumption, making TPUs highly effective for running complex neural network computations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRBvFutTc2uh"
      },
      "source": [
        "---\n",
        "### Q5 Evaluation (15 Points):\n",
        "- How does the TPU's performance compare to contemporary CPUs and GPUs for machine learning tasks?\n",
        "- What benchmarks or workloads were used to evaluate the TPU's performance in the datacenter?\n",
        "- Discuss the TPU's performance per watt. Why is energy efficiency crucial in datacenter environments?\n",
        "- What types of machine learning models and tasks are best suited for TPUs?\n",
        "- How have TPUs impacted the training times of large-scale models at Google?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKUL8nhTdNLZ"
      },
      "source": [
        "### A5:\n",
        "1.  The TPU is on average about 15X to 30X faster than contemporary GPUs and CPUs for machine learning tasks, with energy efficiency (TOPS/Watt) about 30X to 80X higher.\n",
        "2. The TPU's performance was evaluated using real-world neural network applications, including Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Long Short-Term Memories (LSTMs), which represent a significant portion of neural network inference workloads in datacenters.\n",
        "3. TPUs outperform Haswell CPUs and K80 GPUs in performance per watt, making them highly efficient for data centers, crucial for cost and environmental impact reduction.\n",
        "4. TPUs are ideal for models such as Multi-Layer Perceptrons, CNNs, and Recurrent Neural Networks, including Long Short-Term Memory networks.\n",
        "5. TPUs have notably cut down training times for large-scale models at Google. A redesigned TPU could achieve two to three times faster inference speeds, enhancing its performance per watt advantage to nearly 70 times over K80 GPUs and 200 times over Haswell CPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp_FV2bzdV4D"
      },
      "source": [
        "---\n",
        "### Q6 Contextual Understanding (10 Points):\n",
        "- What are some of the challenges faced in deploying TPUs in datacenters?\n",
        "- Are there specific machine learning tasks or models that TPUs might struggle with compared to other hardware?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvpxYhvNdg18"
      },
      "source": [
        "### A6:\n",
        "1. The paper outlines challenges in deploying TPUs in datacenters, such as integrating them with existing infrastructure, ensuring software compatibility, managing power and cooling requirements.\n",
        "2. TPUs may face difficulties with memory-intensive machine learning tasks like some MLPs and LSTMs, while compute-heavy tasks such as CNNs gain more from increased clock speeds. This suggests that TPU performance greatly depends on the specific type of machine learning workload."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxUpnGmtdjfX"
      },
      "source": [
        "---\n",
        "### Q7 Discussion and Critique (10 points):\n",
        "- What are the strengths and weaknesses of the paper's methodology and analysis?\n",
        "- Are there any potential biases or assumptions in the paper that you disagree with or find questionable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjAQy1qLdvZ7"
      },
      "source": [
        "### A7:\n",
        "1. The paper's strengths include a detailed evaluation of the TPU's performance using real-world machine learning workloads, providing a direct comparison with contemporary CPUs and GPUs. It highlights the TPU's superior speed and energy efficiency for neural network inference tasks. Additionally, the paper offers insights into the design rationale behind the TPU and its impact on datacenter cost-performance metrics. However, weaknesses in the paper's methodology and analysis might include a limited scope of comparison, focusing mainly on neural network inference without extensively covering training performance. There's also a potential lack of detailed discussion on the challenges of integrating TPUs into existing datacenter ecosystems and how these might affect scalability and maintenance.\n",
        "2. The paper assumes TPUs outperform CPUs and GPUs in all machine learning tasks, which may overlook each hardware's unique strengths for certain tasks. Also, it focuses mainly on TPUs' in-datacenter performance, possibly underestimating the challenges of integrating them into existing systems and workflows. This approach could bias the analysis towards TPUs' advantages without fully addressing their deployment and compatibility complexities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSMCnPg8d_0S"
      },
      "source": [
        "---\n",
        "### Q8 Reflection (10 Points):\n",
        "- How do TPUs fit into the larger trend of custom hardware accelerators for machine learning, such as FPGAs and ASICs?\n",
        "- Discuss the trade-offs between general-purpose hardware (like CPUs) and specialized hardware (like TPUs) in the context of evolving machine learning workloads.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBtgEgPXeNR5"
      },
      "source": [
        "### A8:\n",
        "1. TPUs highlight the industry's shift towards maximizing cost-efficiency and energy savings for high-demand tasks through the use of domain-specific architectures.\n",
        "2. General-purpose hardware delivers wide-ranging flexibility and programming ease for various tasks but at the expense of higher power usage and less efficiency for specific tasks. In contrast, specialized hardware boosts performance and energy efficiency significantly, though it comes with less flexibility and possibly greater development costs for adapting to new or diverse tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}