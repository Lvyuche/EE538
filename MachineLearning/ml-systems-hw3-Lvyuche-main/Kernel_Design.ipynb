{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j-AqduQQfgJ"
      },
      "source": [
        "# EE 599 HW 3 Part 1: Kernel Design\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxRxx_KuQfgM"
      },
      "source": [
        "## Im2col Algorithm\n",
        "\n",
        "\n",
        "`Im2col` is a method used in CNNs to transform the input data and filters into a format that allows the convolution to be expressed as a matrix multiplication. This transformation can simplify the implementation of convolution and leverage highly optimized matrix multiplication routines such as BLAS library.\n",
        "\n",
        "In the class and discussion, we have covered the `im2col` algorithm for 2D input with 2D filter.\n",
        "\n",
        "In this section, we extend and implement the `im2col` algorithm for 4D input with 4D filters.\n",
        "\n",
        "First, let's import packages, configure the convolution operation, and randomly initialize the input and filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUc57mDKQfgN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "padding = 0\n",
        "stride = 1\n",
        "\n",
        "N, C, H, W = 4, 3, 5, 5\n",
        "K, C, KH, KW = 2, 3, 3, 3\n",
        "\n",
        "input = torch.randn(N, C, H, W)\n",
        "filter = torch.randn(K, C, KH, KW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KmmQFLEQfgN"
      },
      "source": [
        "### **TODO 1:**\n",
        "\n",
        "Calculate the output height `OH` and output width `OW` for the convolution operation, based on the input dimensions, padding, stride, and kernel size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ANErvEoQfgO",
        "outputId": "da615c50-13eb-4dee-d620-8f8feab32c28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "#TODO: compute OH and OW\n",
        "\n",
        "OH = int((H - KH + 2 * padding) / stride) + 1\n",
        "OW = int((W - KW + 2 * padding) / stride) + 1\n",
        "\n",
        "# print(OH)\n",
        "# print(OW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27IpOZHyQfgO"
      },
      "source": [
        "### **TODO 2**:\n",
        "\n",
        "Implement the `im2col` operation to transform the 4D input and filter tensors into two 2D matrices. After matrix multiplication, reshape the result back into a 4D tensor to simulate the convolution operation. Compare your implementation's result `output_im2col` with PyTorch's `conv2d` function to verify correctness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "act8PCDgQfgO",
        "outputId": "26d5a812-9dbb-472b-f7a3-aeb73a4af95d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-11.3348,  -9.5617,  -7.9537],\n",
            "          [  6.9408,  -3.1637,   9.1057],\n",
            "          [  1.8903,  -4.2099,  -2.9147]],\n",
            "\n",
            "         [[ -5.5125,  -1.3172,   2.6162],\n",
            "          [  2.7180,  -3.5951,   5.0873],\n",
            "          [ -0.7070,   0.2077,  -3.5802]]],\n",
            "\n",
            "\n",
            "        [[[ 12.0494,  -2.2278,   1.7364],\n",
            "          [  8.8223,  -1.1798,   5.5721],\n",
            "          [  5.0131,   2.6206,   9.0075]],\n",
            "\n",
            "         [[ -5.3580,   6.4889,   1.7364],\n",
            "          [  3.3177,   4.6009,  -3.4258],\n",
            "          [  0.6937,   2.3984,  -5.4314]]],\n",
            "\n",
            "\n",
            "        [[[ -3.1373,  -3.8238,   9.4231],\n",
            "          [  0.3158,  -9.6695,   0.2259],\n",
            "          [  3.5829,  -7.3342,  -2.6818]],\n",
            "\n",
            "         [[ -5.1266,   2.6498,  10.4558],\n",
            "          [ -1.4673,  -5.7154,   3.7507],\n",
            "          [ -0.6576,  -4.8772,  -4.0104]]],\n",
            "\n",
            "\n",
            "        [[[ -0.6623,  -5.4126,  -2.8191],\n",
            "          [  3.9214,   3.2033,   2.1995],\n",
            "          [  6.7370,   2.2119,  -1.6412]],\n",
            "\n",
            "         [[ -6.0497,   5.9986, -10.4230],\n",
            "          [  2.8818,  -7.1503,   2.0619],\n",
            "          [  3.2120,   4.8137,   4.2063]]]])\n"
          ]
        }
      ],
      "source": [
        "#TODO: implement im2col for 4D input and filter\n",
        "\n",
        "toeplitz_input = torch.zeros(C * KH * KW, OH * OW)\n",
        "toeplitz_filter = torch.zeros(K, C * KH * KW)\n",
        "\n",
        "block_size = KH * KW\n",
        "\n",
        "[filter_row, filter_col] = toeplitz_filter.size()\n",
        "[input_row, input_col] = toeplitz_input.size()\n",
        "\n",
        "col = 0\n",
        "for k in range(K):\n",
        "  for c in range(C):\n",
        "    block = filter[k, c, :, :].flatten()\n",
        "    toeplitz_filter[k, c * block_size : (c + 1) * block_size] = block\n",
        "# print(toeplitz_filter.size())\n",
        "\n",
        "for n in range(N):\n",
        "  col = 0\n",
        "  for i in range(OH):\n",
        "    for j in range(OW):\n",
        "      for c in range(C):\n",
        "        block = input[n, c, i: i + KH, j: j + KW].flatten()\n",
        "        toeplitz_input[c * block_size: c * block_size + block_size, col] = block\n",
        "      col += 1\n",
        "  # print(toeplitz_input.size())\n",
        "\n",
        "  temp = torch.matmul(toeplitz_filter, toeplitz_input)\n",
        "  temp = temp.view(-1, K, OH, OW)\n",
        "\n",
        "  if(n == 0):\n",
        "    output_im2col = temp\n",
        "  else:\n",
        "    output_im2col = torch.cat((output_im2col, temp), dim=0)\n",
        "\n",
        "print(output_im2col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKeMcZUBQfgO"
      },
      "source": [
        "The correct result is provided below by using Pytorch's `conv2d` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QHGI8DsQfgO",
        "outputId": "e46e89ce-6f0f-427e-da2e-a200528a2d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-11.3348,  -9.5617,  -7.9537],\n",
            "          [  6.9408,  -3.1637,   9.1057],\n",
            "          [  1.8903,  -4.2099,  -2.9147]],\n",
            "\n",
            "         [[ -5.5125,  -1.3172,   2.6162],\n",
            "          [  2.7180,  -3.5951,   5.0873],\n",
            "          [ -0.7070,   0.2077,  -3.5802]]],\n",
            "\n",
            "\n",
            "        [[[ 12.0494,  -2.2278,   1.7364],\n",
            "          [  8.8223,  -1.1798,   5.5721],\n",
            "          [  5.0131,   2.6206,   9.0075]],\n",
            "\n",
            "         [[ -5.3580,   6.4889,   1.7364],\n",
            "          [  3.3177,   4.6009,  -3.4258],\n",
            "          [  0.6937,   2.3984,  -5.4314]]],\n",
            "\n",
            "\n",
            "        [[[ -3.1373,  -3.8238,   9.4231],\n",
            "          [  0.3158,  -9.6695,   0.2259],\n",
            "          [  3.5829,  -7.3342,  -2.6818]],\n",
            "\n",
            "         [[ -5.1266,   2.6498,  10.4558],\n",
            "          [ -1.4673,  -5.7154,   3.7507],\n",
            "          [ -0.6576,  -4.8772,  -4.0104]]],\n",
            "\n",
            "\n",
            "        [[[ -0.6623,  -5.4125,  -2.8191],\n",
            "          [  3.9214,   3.2033,   2.1995],\n",
            "          [  6.7370,   2.2119,  -1.6412]],\n",
            "\n",
            "         [[ -6.0497,   5.9986, -10.4230],\n",
            "          [  2.8818,  -7.1503,   2.0619],\n",
            "          [  3.2120,   4.8137,   4.2063]]]])\n"
          ]
        }
      ],
      "source": [
        "output_conv2d = F.conv2d(input, filter, stride=stride, padding=padding)\n",
        "print(output_conv2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zpoZJpMQfgO"
      },
      "source": [
        "We can use the following functions to check how many the elements are matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkrdQLNcQfgP",
        "outputId": "ea2fc2c6-0cad-4b75-b9e0-d8ce9c016e3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of matched elements = 72\n",
            "Number of total elements = 72\n"
          ]
        }
      ],
      "source": [
        "# print sum of the True values\n",
        "print(\"Number of matched elements =\", torch.isclose(output_im2col, output_conv2d).sum().item())\n",
        "\n",
        "# print the total number of values\n",
        "print(\"Number of total elements =\", output_im2col.numel())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo7KP9sNQfgP"
      },
      "source": [
        "## Matrix Multiplication Optimization\n",
        "\n",
        "While matrix multiplication operations are foundational, they can pose challenges, especially in terms of expensive computational complexity.\n",
        "Present compilers are incapable of fully harnessing the processor architecture complexity. There is a wide gap between the available and achieved performance of software. Thereby, the need for performance tuning. Performance tuning of the simple matrix multiplication has indeed been a very tough and challenging project. In this section, we discuss some of the optimization techniques, which gave us substantial improvements.\n",
        "\n",
        "To evaluate and improve the performance of matrix multiplication implementations, it's beneficial to use low-level programming languages like C or C++, which offer closer control over hardware resources. Within a notebook environment, we can facilitate the development, compilation, and execution of C code by using specific commands. The `%%writefile` command allows us to save the content of a notebook cell directly into a file, which can then be compiled and executed using command-line instructions.\n",
        "\n",
        "In the cell below, we provide a naive matrix multiplication implementation and measure the FLOPs per section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zhoaUBe4QfgP",
        "outputId": "e0864ec7-8643-4d5d-baf1-4ec00f7927e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing naive_mm.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile naive_mm.c\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "\n",
        "int main(void) {\n",
        "    int i, j, k;\n",
        "    struct timespec start, stop;\n",
        "    double time;\n",
        "    int n = 1024; // Matrix size is n*n\n",
        "\n",
        "    // Allocate memory for matrices A, B, and C\n",
        "    double **A = (double**) malloc(sizeof(double*) * n);\n",
        "    double **B = (double**) malloc(sizeof(double*) * n);\n",
        "    double **C = (double**) malloc(sizeof(double*) * n);\n",
        "    for (i = 0; i < n; i++) {\n",
        "        A[i] = (double*) malloc(sizeof(double) * n);\n",
        "        B[i] = (double*) malloc(sizeof(double) * n);\n",
        "        C[i] = (double*) malloc(sizeof(double) * n);\n",
        "    }\n",
        "\n",
        "    // Initialize matrices A and B\n",
        "    for (i = 0; i < n; i++) {\n",
        "        for (j = 0; j < n; j++) {\n",
        "            A[i][j] = i;\n",
        "            B[i][j] = i + j;\n",
        "            C[i][j] = 0;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Start timer\n",
        "    if (clock_gettime(CLOCK_REALTIME, &start) == -1) {\n",
        "        perror(\"clock gettime\");\n",
        "    }\n",
        "\n",
        "    // Naive Matrix Multiplication\n",
        "    for (i = 0; i < n; i++) {\n",
        "        for (j = 0; j < n; j++) {\n",
        "            for (k = 0; k < n; k++) {\n",
        "                C[i][j] = C[i][j] + A[i][k] * B[k][j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Stop timer\n",
        "    if (clock_gettime(CLOCK_REALTIME, &stop) == -1) {\n",
        "        perror(\"clock gettime\");\n",
        "    }\n",
        "    time = (stop.tv_sec - start.tv_sec) + (double)(stop.tv_nsec - start.tv_nsec) / 1e9;\n",
        "\n",
        "    // Print results\n",
        "    printf(\"Number of FLOPs = %u, Execution time = %f sec,\\n%lf MFLOPs per sec\\n\", 2 * n * n * n, time, 1 / time / 1e6 * 2 * n * n * n);\n",
        "    printf(\"C[100][100]=%f\\n\", C[100][100]);\n",
        "\n",
        "    // Release memory\n",
        "    for (i = 0; i < n; i++) {\n",
        "        free(A[i]);\n",
        "        free(B[i]);\n",
        "        free(C[i]);\n",
        "    }\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irL5EbTJQfgP"
      },
      "source": [
        "Compile and execute the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16_hjfaQQfgP",
        "outputId": "c0785f7d-fa38-4e27-ed0a-027b0cf9308f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of FLOPs = 2147483648, Execution time = 22.409222 sec,\n",
            "95.830352 MFLOPs per sec\n",
            "C[100][100]=62617600.000000\n"
          ]
        }
      ],
      "source": [
        "!g++ naive_mm.c -o naive_mm && ./naive_mm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-SlWNV1QfgP"
      },
      "source": [
        "### **TODO 3:**\n",
        "\n",
        "Blocked matrix multiplication, also known as tiled matrix multiplication, is an optimization technique used to improve the performance of matrix multiplication operations, especially on modern hardware with hierarchical memory systems. This approach involves dividing the input matrices into smaller sub-matrices or \"blocks\" and then performing the multiplication on these blocks rather than on individual elements.\n",
        "\n",
        "In the cell below, fill out the missing code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mw8JyMiGQfgP",
        "outputId": "a95335e1-dadd-4e0f-a58e-52f7e0a656c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing blocking_mm.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile blocking_mm.c\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int i, j, k, ii, jj, kk;\n",
        "    struct timespec start, stop;\n",
        "    double time;\n",
        "    int n = 1024; // Matrix size is n*n\n",
        "    int b = atoi(argv[1]); // Block size\n",
        "\n",
        "    // Allocate memory for matrices A, B, and C\n",
        "    double **A = (double**) malloc(sizeof(double*) * n);\n",
        "    double **B = (double**) malloc(sizeof(double*) * n);\n",
        "    double **C = (double**) malloc(sizeof(double*) * n);\n",
        "    for (i = 0; i < n; i++) {\n",
        "        A[i] = (double*) malloc(sizeof(double) * n);\n",
        "        B[i] = (double*) malloc(sizeof(double) * n);\n",
        "        C[i] = (double*) malloc(sizeof(double) * n);\n",
        "    }\n",
        "\n",
        "    // Initialize matrices A and B\n",
        "    for (i = 0; i < n; i++) {\n",
        "        for (j = 0; j < n; j++) {\n",
        "            A[i][j] = i;\n",
        "            B[i][j] = i + j;\n",
        "            C[i][j] = 0;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Start timer\n",
        "    if (clock_gettime(CLOCK_REALTIME, &start) == -1) {\n",
        "        perror(\"clock gettime\");\n",
        "    }\n",
        "\n",
        "    // TODO: Blocking Matrix Multiplication\n",
        "    // Your code goes here\n",
        "    //*******************************//\n",
        "    for (ii = 0; ii < n; ii += b) {\n",
        "      for (jj = 0; jj < n; jj += b) {\n",
        "        for (kk = 0; kk < n; kk += b) {\n",
        "            for (i = ii; i < ii + b && i < n; i++) {\n",
        "                for (j = jj; j < jj + b && j < n; j++) {\n",
        "                    for (k = kk; k < kk + b && k < n; k++) {\n",
        "                        C[i][j] = C[i][j] + A[i][k] * B[k][j];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    //*******************************//\n",
        "\n",
        "    // Stop timer\n",
        "    if (clock_gettime(CLOCK_REALTIME, &stop) == -1) {\n",
        "        perror(\"clock gettime\");\n",
        "    }\n",
        "    time = (stop.tv_sec - start.tv_sec) + (double)(stop.tv_nsec - start.tv_nsec) / 1e9;\n",
        "\n",
        "    // Print results\n",
        "    printf(\"Number of FLOPs = %u, Execution time = %f sec,\\n%lf MFLOPs per sec\\n\", 2 * n * n * n, time, 1 / time / 1e6 * 2 * n * n * n);\n",
        "    printf(\"C[100][100]=%f\\n\", C[100][100]);\n",
        "\n",
        "    // Release memory\n",
        "    for (i = 0; i < n; i++) {\n",
        "        free(A[i]);\n",
        "        free(B[i]);\n",
        "        free(C[i]);\n",
        "    }\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNUtNzA_QfgP"
      },
      "source": [
        "Compile and execute the code with different block sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V68umzd2QfgQ",
        "outputId": "b561d2cd-9645-4c58-914b-1f63912210a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of FLOPs = 2147483648, Execution time = 11.591916 sec,\n",
            "185.257011 MFLOPs per sec\n",
            "C[100][100]=62617600.000000\n",
            "Number of FLOPs = 2147483648, Execution time = 9.187708 sec,\n",
            "233.734438 MFLOPs per sec\n",
            "C[100][100]=62617600.000000\n",
            "Number of FLOPs = 2147483648, Execution time = 8.284142 sec,\n",
            "259.228257 MFLOPs per sec\n",
            "C[100][100]=62617600.000000\n"
          ]
        }
      ],
      "source": [
        "!g++ blocking_mm.c -o blocking_mm && ./blocking_mm 4 && ./blocking_mm 8 && ./blocking_mm 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkGcOIM4QfgQ"
      },
      "source": [
        "### **TODO 4:**\n",
        "\n",
        "OpenMP is a powerful API designed for parallel programming in C, C++, and Fortran, enabling efficient utilization of multicore and multiprocessor systems. It simplifies the development of parallel applications by providing a set of straightforward compiler directives, library routines, and environment variables that abstract away the complexities of thread management and synchronization. By allowing code to be parallelized with minimal modifications, OpenMP fosters portability and scalability across various platforms.\n",
        "\n",
        "In the cell below, use the proper pragma configuaration to execute your for loops in parallel. You need to make sure the index is a private variable to each thread, otherwise race conditions might happen. We use the default number of threads in Colab enviroment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZWxz0LqWQfgQ",
        "outputId": "4301a1ea-2865-4767-ef80-0478040b0603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing blocking_mt_mm.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile blocking_mt_mm.c\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <time.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int i, j, k, ii, jj, kk;\n",
        "    struct timespec start, stop;\n",
        "    double time;\n",
        "    int n = 1024; // Matrix size is n*n\n",
        "    int b = atoi(argv[1]); // Block size\n",
        "\n",
        "    // Allocate memory for matrices A, B, and C\n",
        "    double **A = (double**) malloc(sizeof(double*) * n);\n",
        "    double **B = (double**) malloc(sizeof(double*) * n);\n",
        "    double **C = (double**) malloc(sizeof(double*) * n);\n",
        "    for (i = 0; i < n; i++) {\n",
        "        A[i] = (double*) malloc(sizeof(double) * n);\n",
        "        B[i] = (double*) malloc(sizeof(double) * n);\n",
        "        C[i] = (double*) malloc(sizeof(double) * n);\n",
        "    }\n",
        "\n",
        "    // Initialize matrices A and B\n",
        "    for (i = 0; i < n; i++) {\n",
        "        for (j = 0; j < n; j++) {\n",
        "            A[i][j] = i;\n",
        "            B[i][j] = i + j;\n",
        "            C[i][j] = 0;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Start timer\n",
        "    if (clock_gettime(CLOCK_REALTIME, &start) == -1) {\n",
        "        perror(\"clock gettime\");\n",
        "    }\n",
        "\n",
        "    // TODO: Blocking Matrix Multiplication with OpenMP\n",
        "    // Your code goes here\n",
        "    //*******************************//\n",
        "    #pragma omp parallel for private(i, j, k, ii, jj, kk) shared(A, B, C)\n",
        "    for (ii = 0; ii < n; ii += b) {\n",
        "        for (jj = 0; jj < n; jj += b) {\n",
        "            for (kk = 0; kk < n; kk += b) {\n",
        "                for (i = ii; i < ii + b && i < n; i++) {\n",
        "                    for (j = jj; j < jj + b && j < n; j++) {\n",
        "                        for (k = kk; k < kk + b && k < n; k++) {\n",
        "                            C[i][j] += A[i][k] * B[k][j];\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    //*******************************//\n",
        "\n",
        "    // Stop timer\n",
        "    if (clock_gettime(CLOCK_REALTIME, &stop) == -1) {\n",
        "        perror(\"clock gettime\");\n",
        "    }\n",
        "    time = (stop.tv_sec - start.tv_sec) + (double)(stop.tv_nsec - start.tv_nsec) / 1e9;\n",
        "\n",
        "    // Print results\n",
        "    printf(\"Number of FLOPs = %u, Execution time = %f sec,\\n%lf MFLOPs per sec\\n\", 2 * n * n * n, time, 1 / time / 1e6 * 2 * n * n * n);\n",
        "    printf(\"C[100][100]=%f\\n\", C[100][100]);\n",
        "\n",
        "    // Release memory\n",
        "    for (i = 0; i < n; i++) {\n",
        "        free(A[i]);\n",
        "        free(B[i]);\n",
        "        free(C[i]);\n",
        "    }\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ2BTBJXQfgQ"
      },
      "source": [
        "Compile and execute the code with different block sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wLwAuE_mQfgQ",
        "outputId": "cb4321b8-49aa-42df-a3ac-d66d4f1fc568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of FLOPs = 2147483648, Execution time = 9.577955 sec,\n",
            "224.211083 MFLOPs per sec\n",
            "C[100][100]=62617600.000000\n",
            "Number of FLOPs = 2147483648, Execution time = 8.163059 sec,\n",
            "263.073407 MFLOPs per sec\n",
            "C[100][100]=62617600.000000\n",
            "Number of FLOPs = 2147483648, Execution time = 6.489922 sec,\n",
            "330.895120 MFLOPs per sec\n",
            "C[100][100]=62617600.000000\n"
          ]
        }
      ],
      "source": [
        "!g++ -fopenmp blocking_mt_mm.c -o blocking_mt_mm && ./blocking_mt_mm 4 && ./blocking_mt_mm 8 && ./blocking_mt_mm 16"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}