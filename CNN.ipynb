{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc3kdYUUlfGP"
      },
      "source": [
        "# EE 599 HW 2: Convolution Neural Network\n",
        "\n",
        "Your task in this Colab notebook is to fill out the sections that are specified by **TODO** (please search the keyword `TODO` to make sure you do not miss any).\n",
        "\n",
        "Prerequisites: set the runtime type to GPU. (Runtime -> Change Runtime Type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hbiiMcdNJI--"
      },
      "outputs": [],
      "source": [
        "#%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG3WW1Owq1Fh"
      },
      "source": [
        "Verify that GPU is availble to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A3RF5VmcoUMG",
        "outputId": "0a93cf60-d1d3-4487-c334-5c239bdb8ee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjKQYob-hDrw"
      },
      "source": [
        "## Convolution Operation\n",
        "\n",
        "Definition:\n",
        "Convolution is a mathematical operation where a filter (or kernel) is applied over an input (e.g., image) to produce a modified output known as a feature map.\n",
        "\n",
        "Key Components:\n",
        "- Kernel/Filter: A small matrix that slides over the input. It captures patterns or features from the input.\n",
        "  \n",
        "- Stride: The number of pixels the filter moves at each step.\n",
        "  \n",
        "- Padding: Zeros added around the input's border, often used to control the output's spatial dimensions.\n",
        "\n",
        "Operation:\n",
        "The filter starts at the top-left corner of the image. For each position, values under the filter are multiplied with its corresponding filter values, then summed to produce a single pixel in the output. This process is repeated across the entire input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOR3xVytYchU"
      },
      "source": [
        "## Create a sample image\n",
        "\n",
        "Create a 3D  `image` of shape `(1,10,10)` as a numpy-array. The signal values are 1 (white) in the `(4,4)`-center region and 0 (black) elsewhere.\n",
        "\n",
        "Plot the image as a grid with black or white colors and write the 0 or 1 value on each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv5aiZYjhVMj",
        "outputId": "92c2daa5-2765-43d8-ba83-17736b3bb8dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 1. 1. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 1. 1. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 1. 1. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 1. 1. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
            "(1, 10, 10)\n"
          ]
        }
      ],
      "source": [
        "# Create a 1x10x10 image filled with zeros\n",
        "image = np.zeros((1, 10, 10), dtype=np.float32)\n",
        "\n",
        "# Set a 4x4 sub-matrix (starting from index 3 to 7 both for rows and columns) to be filled with ones\n",
        "image[0, 3:7, 3:7] = 1\n",
        "\n",
        "# Print the formatted matrix\n",
        "print(image)\n",
        "print(image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "HWt-4xoXhqb9",
        "outputId": "f6df94ea-da61-4e19-843b-9c124d3928be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def plot_image_with_values(image, title):\n",
        "    \"\"\"\n",
        "    Plots a 3D image with numeric values overlaid on each cell.\n",
        "\n",
        "    Parameters:\n",
        "    - image (3D numpy array): The image to be plotted.\n",
        "    - title (str): The title of the plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(5,5))\n",
        "\n",
        "    # Plot the image\n",
        "    plt.imshow(image[0], cmap='gray')\n",
        "\n",
        "    # Looping through all cells to overlay the numeric values\n",
        "    for i in range(image.shape[1]):\n",
        "        for j in range(image.shape[2]):\n",
        "            pixel_value = image[0, i, j]\n",
        "            text_color = 'white' if pixel_value < 0.5 else 'black'  # Setting text color based on pixel value\n",
        "            plt.text(j, i, f'{pixel_value:.2f}', ha='center', va='center', color=text_color)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_image_with_values(image, 'A sample image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqQLUTPfi0JZ"
      },
      "source": [
        "## Average Filter\n",
        "\n",
        "### **TODO 1**:\n",
        "Implement a function that applies a 3x3 average filter to an input image using PyTorch's [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function. The filter should have all values set to 1/9, ensuring the sum is 1, and use a padding of 1 to keep the output image dimensions the same as the input.\n",
        "\n",
        "Note that the input and output of the `average_filter` function are numpy array type, so you need to convert numpy array to pytorch tensor, process the tensor, and convert pytorch tensor back to numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVgk7qEuizVj"
      },
      "outputs": [],
      "source": [
        "def average_filter(image):\n",
        "    \"\"\"\n",
        "    Apply a 3x3 average filter to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "    tensor = torch.from_numpy(image);\n",
        "\n",
        "    return filtered_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB0IBoQ6P2jP"
      },
      "source": [
        "Plot the image as a grid with black or white colors and write the 0 or 1 value on each cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L6R21Z4P2jP"
      },
      "outputs": [],
      "source": [
        "filtered_img = average_filter(image)\n",
        "plot_image_with_values(filtered_img, 'Average Filtered Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14_iJ0ZnZJi"
      },
      "source": [
        "## Edge Detection\n",
        "\n",
        "In image processing, edge detection refers to the process of identifying abrupt changes or discontinuities in an image. These abrupt changes often correspond to object boundaries, textures, or other significant features.\n",
        "\n",
        "The concept of **central differences** can be applied to images to detect edges by estimating the gradient magnitude at each pixel. In the context of images, the derivative approximations can highlight the areas of rapid intensity change, which are often the edges.\n",
        "\n",
        "Using the concept of central differences, one possible filter for vertical edge detection is:\n",
        "\\begin{bmatrix}\n",
        "-1 & 0 & 1 \\\\\n",
        "-1 & 0 & 1 \\\\\n",
        "-1 & 0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "### **TODO 2**:\n",
        "Implement a function that applies a 3x3 vertical edge detection filter to an input image using PyTorch's [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGrPkCHOP2jP"
      },
      "outputs": [],
      "source": [
        "def vertical_edge_detection(image):\n",
        "    \"\"\"\n",
        "    Apply a 3x3 vertical edge detection filter to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXtXywfxP2jQ"
      },
      "outputs": [],
      "source": [
        "filtered_img = vertical_edge_detection(image)\n",
        "plot_image_with_values(filtered_img, 'Vertical Edge Detection Filter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbxxvlRdp20x"
      },
      "source": [
        "## Horizontal Edge\n",
        "\n",
        "### **TODO 3:**\n",
        "Implement a function that applies a 3x3 horizontal edge detection filter to an input image using PyTorch's [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function. What should the filter be in this case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAydg700qLsw"
      },
      "outputs": [],
      "source": [
        "def horizontal_edge_detection(image):\n",
        "    \"\"\"\n",
        "    Apply a 3x3 horizontal edge detection filter to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Its3_KIkP2jQ"
      },
      "outputs": [],
      "source": [
        "filtered_img = horizontal_edge_detection(image)\n",
        "plot_image_with_values(filtered_img, 'Horizontal Edge Detection Filter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kle9PjhE8wXY"
      },
      "source": [
        "## Laplacian Sharpening\n",
        "Lapalacian filters can increase the regions of the image with rapid intensity. It is very helpful in hiding the fine details of images. Any features with a sharp discontunioty will be enhanced.\n",
        "\n",
        "One of the Laplacian filter is\n",
        "\n",
        "\\begin{bmatrix}\n",
        " 0 & -1 &  0 \\\\\n",
        "-1 &  5 & -1 \\\\\n",
        " 0 & -1 &  0 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "\n",
        "### **TODO 4**:\n",
        "Implement a function that applies a 3x3 laplacian sharpening filter to an input image using PyTorch's [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2w_onqCP2jQ"
      },
      "outputs": [],
      "source": [
        "def laplacian_filter(image):\n",
        "    \"\"\"\n",
        "    Apply a 3x3 laplacian filter to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMwLyA3oP2jQ"
      },
      "outputs": [],
      "source": [
        "filtered_img = laplacian_filter(image)\n",
        "plot_image_with_values(filtered_img, 'Laplacian Filter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcXeVjszchQz"
      },
      "source": [
        "## Applying Filters on a Real Image\n",
        "\n",
        "In this section we apply several filters on a real image.\n",
        "\n",
        "Use `PIL` library to read a image from online resource called `sheep.jpg` as a numpy array. Convert that image to grayscale so that it only has a single channel and plot the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dPoNC39r8zS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import urllib.request\n",
        "\n",
        "image = Image.open(urllib.request.urlopen('https://chokkan.github.io/deeplearning/assets/images/sheep.jpg'))\n",
        "image = image.resize((256, 256))  # resize image.\n",
        "plt.imshow(image)\n",
        "\n",
        "image = image.convert('L')  # convert to grey scale\n",
        "image = np.array(image, dtype=np.float32) / 255  # normalize image to range [0, 1]\n",
        "image = np.expand_dims(image, axis=0)  # expand to 3 dimensional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2BgcX5oP2jQ"
      },
      "source": [
        "Create a function that can plot both the original image and its modified version side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIR5fjl7P2jQ"
      },
      "outputs": [],
      "source": [
        "def plot_original_and_modified(original_img, modified_img):\n",
        "    \"\"\"\n",
        "    Plot the original image and its modified version side by side.\n",
        "\n",
        "    Args:\n",
        "    - original_img: A 3D numpy array representing the original image.\n",
        "\n",
        "    - modified_img: A 3D numpy array representing the the modified version of the original image.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure that the input arrays are in the range [0, 1]\n",
        "    original_img = np.clip(original_img, 0, 1)[0]\n",
        "    modified_img = np.clip(modified_img, 0, 1)[0]\n",
        "\n",
        "    plt.figure(num=None, figsize=(12,12), dpi=80, facecolor='w', edgecolor='k')\n",
        "\n",
        "    # Plot the original image\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(original_img, cmap='Greys_r')\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    # Plot the modified image\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(modified_img, cmap='Greys_r')\n",
        "    plt.title(\"Modified Image\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcRb4WApdJHt"
      },
      "source": [
        "### **TODO 5**:\n",
        "Implement a function that applies a 3x3 vertical edge detection filter to an input image using PyTorch's [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPY1cdIBP2jR"
      },
      "outputs": [],
      "source": [
        "def vertical_edge_detection(image):\n",
        "    \"\"\"\n",
        "    Apply a 3x3 vertical edge detection filter to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img\n",
        "\n",
        "filtered_img = vertical_edge_detection(image)\n",
        "plot_original_and_modified(image, filtered_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbCgGFpudPyA"
      },
      "source": [
        "### **TODO 6:**\n",
        "Implement a function that applies a 3x3 horizontal edge detection filter to an input image using PyTorch's [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsL3jjKXuCIz"
      },
      "outputs": [],
      "source": [
        "def horizontal_edge_detection(image):\n",
        "    \"\"\"\n",
        "    Apply a 3x3 horizontal edge detection filter to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img\n",
        "\n",
        "filtered_img = horizontal_edge_detection(image)\n",
        "plot_original_and_modified(image, filtered_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXOXAXCU_Skn"
      },
      "source": [
        "### **TODO 7:**\n",
        "Implement a function that applies a 3x3 laplacian filter to an input image using PyTorch's [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knmKRg_X_NTp"
      },
      "outputs": [],
      "source": [
        "def laplacian_filter(image):\n",
        "    \"\"\"\n",
        "    Apply a 3x3 laplacian filter to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img\n",
        "\n",
        "filtered_img = laplacian_filter(image)\n",
        "plot_original_and_modified(image, filtered_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhB85-1NNfGP"
      },
      "source": [
        "### Gaussian Filter for Blurring\n",
        "\n",
        "The Gaussian filter is commonly used for blurring image. Its values are determined by the Gaussian function, resulting in a weighted average that emphasizes the central pixel and decreases the influence of neighboring pixels as the distance increases.\n",
        "\n",
        "The specific weights for the 3x3 Gaussian filter are arranged as follows, ensuring that the sum of all weights is equal to 1, which preserves the overall brightness of the image:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1/16 & 2/16 & 1/16 \\\\\n",
        "2/16 & 4/16 & 2/16 \\\\\n",
        "1/16 & 2/16 & 1/16 \\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "This arrangement of weights creates a bell-shaped curve, centralizing the focus on the middle pixel while progressively giving less importance to pixels further away.\n",
        "\n",
        "### **TODO 8:**\n",
        "Implement a function that applies a 3x3 gaussian filter to an input image using PyTorch's [conv2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKodFHxQNtaI"
      },
      "outputs": [],
      "source": [
        "def gaussian_filter(image):\n",
        "    \"\"\"\n",
        "    Apply a 3x3 gaussian filter to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img\n",
        "\n",
        "filtered_img = gaussian_filter(image)\n",
        "plot_original_and_modified(image, filtered_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "allKLfTvSHmv"
      },
      "source": [
        "## Pooling\n",
        "\n",
        "Pooling is an operation frequently used in the context of Convolutional Neural Networks (CNNs) for image processing. The main idea behind pooling is to reduce the spatial size of the representation, thereby reducing the number of parameters and computational complexity in the network. This helps in making the network less prone to overfitting and also reduces computation time.\n",
        "\n",
        "There are several types of pooling operations, with the most common ones being:\n",
        "\n",
        "**Max Pooling**: In this method, for each segment of the input, the maximum value is selected. For instance, if we consider a 2x2 pooling window (often called a \"kernel\" in this context) and slide it over an image tensor, the largest value within each 2x2 window will be selected for the pooled output.\n",
        "\n",
        "**Average Pooling**: Instead of taking the maximum value in each window, average pooling takes the average value. So for a 2x2 window, it would take the average of all 4 values in that window.\n",
        "\n",
        "**Min Pooling**: This operation selects the minimum value from each window, though it's less commonly used than max or average pooling.\n",
        "\n",
        "**Global Pooling**: Instead of using a fixed size window, global pooling operates over the entire height and width of the feature map, effectively transforming the spatial dimensions of the feature map to 1x1 (retaining the depth/channels).\n",
        "\n",
        "The main benefits of pooling include:\n",
        "\n",
        "- **Dimensionality Reduction**: It reduces the spatial dimensions of the feature maps, which in turn reduces the number of parameters in subsequent layers, leading to faster computations.\n",
        "- **Invariance to Small Translations**: Small translations in the input image might not lead to significant changes in the pooled feature map. This can be particularly useful when the exact location of features is less relevant than their presence.\n",
        "- **Prevents Overfitting**: By reducing the spatial resolution and thus the total number of parameters, pooling can help prevent overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL0GQXY6UYSp"
      },
      "source": [
        "### **TODO 9:**\n",
        "Implement a function that applies a 5x5 max pooling filter with stride 5 to an input image using PyTorch's [max_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBmFUrJQR8Yf"
      },
      "outputs": [],
      "source": [
        "def max_pooling(image):\n",
        "    \"\"\"\n",
        "    Apply a 5x5 max pooling to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img\n",
        "\n",
        "filtered_img = max_pooling(image)\n",
        "plot_original_and_modified(image, filtered_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKgFIc4bW4_y"
      },
      "source": [
        "### **TODO 10:**\n",
        "Implement a function that applies a 5x5 average pooling filter with stride 5 to an input image using PyTorch's [avg_pool2d](https://pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool2d.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSZunSLCWXca"
      },
      "outputs": [],
      "source": [
        "def average_pooling(image):\n",
        "    \"\"\"\n",
        "    Apply a 5x5 average pooling to the input image using PyTorch and return the filtered image.\n",
        "\n",
        "    Args:\n",
        "    - image: A 3D numpy array representing the input image.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_img: A 3D numpy array representing the image after applying the filter.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    return filtered_img\n",
        "\n",
        "filtered_img = average_pooling(image)\n",
        "plot_original_and_modified(image, filtered_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaMDWYArEXO"
      },
      "source": [
        "## Convolutional Neural Networks\n",
        "\n",
        "The construction of neural networks is facilitated through the utilization of the ``torch.nn`` package within the PyTorch framework.\n",
        "\n",
        "A standard process for training a neural network involves the following steps:\n",
        "\n",
        "1. **Neural Network Definition:** The first step entails defining the architecture of the neural network, which encompasses the design of its interconnected layers and the inclusion of learnable parameters, often referred to as weights.\n",
        "\n",
        "2. **Iterative Data Processing:** Next, the training procedure involves iterating through a dataset containing input samples. Each input is fed into the neural network for processing.\n",
        "\n",
        "3. **Forward Propagation:** During this phase, the input data is passed through the neural network's layers, activating its neurons through weighted connections and producing an output prediction.\n",
        "\n",
        "4. **Loss Computation:** The output generated by the network is then compared to the actual desired output, leading to the computation of a loss value. This loss signifies the discrepancy between the predicted outcome and the true value.\n",
        "\n",
        "5. **Backpropagation:** After calculating the loss, the gradients of the loss with respect to the network's parameters are computed through a process called backpropagation. This entails tracing how changes in the parameters affect the loss.\n",
        "\n",
        "6. **Gradient Update:** The gradients computed during backpropagation are used to adjust the network's weights. A common method involves updating the weights by subtracting a scaled gradient value from the current weights. The scale factor, known as the learning rate, determines the step size of the update. This update rule can be expressed as: ``weight = weight - learning_rate * gradient``.\n",
        "\n",
        "Throughout this iterative process, the neural network's parameters are fine-tuned to minimize the loss and enhance its predictive accuracy. This sequence of steps forms the backbone of training neural networks, enabling them to learn and improve their performance over time.\n",
        "\n",
        "In this assignment we will create a simple CNN.\n",
        "\n",
        "\n",
        "To begin this section, we will load the CIFAR10 dataset and train a simple convolutional neural network (CNN) to classify the images.\n",
        "\n",
        "The CIFAR10 dataset has the classes:\n",
        "\n",
        "* `airplane`\n",
        "* `automobile`\n",
        "* `bird`\n",
        "* `cat`\n",
        "* `deer`\n",
        "* `dog`\n",
        "* `frog`\n",
        "* `horse`\n",
        "* `ship`\n",
        "* `truck`\n",
        "\n",
        "The images in CIFAR-10 are of size:\n",
        "\n",
        "* `3x32x32`\n",
        "\n",
        "i.e. 3-channel color images of `32x32` pixels in size.\n",
        "\n",
        "To load the data, we use a package called\n",
        "``torchvision``, that has data loaders for common datasets such as\n",
        "ImageNet, CIFAR10, MNIST, etc. and data transformers for images, namely,\n",
        "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
        "\n",
        "\n",
        "In the following code, we perform the following:\n",
        "\n",
        "* Define a batch size for processing multiple images in each iteration.\n",
        "* Create a series of transformations to be applied to the images in the dataset: converting images to tensors and normalizing the pixel values.\n",
        "* Load the CIFAR10 **training** and **test** datasets, specifying the root directory, training mode, and transformations.\n",
        "* Construct a DataLoader for the **training** and **test** datasets with batch processing, shuffling, and parallel data loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5UuOjjrnogR"
      },
      "outputs": [],
      "source": [
        "# Set the batch size for data processing\n",
        "batch_size = 4\n",
        "\n",
        "# Define the mean values and standard deviation values for normalization\n",
        "mean_values = (0.5, 0.5, 0.5)  # Mean values for red, green, and blue channels\n",
        "std_values = (0.5, 0.5, 0.5)   # Standard deviation values for red, green, and blue channels\n",
        "\n",
        "# Define the transformation pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                  # Convert images to tensors\n",
        "    transforms.Normalize(mean_values, std_values)  # Normalize tensor values\n",
        "])\n",
        "\n",
        "# Load the CIFAR10 training dataset and apply the defined transformations\n",
        "trainset = torchvision.datasets.CIFAR10(root=\"./\", train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# Create a DataLoader to efficiently load and process training data in batches\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# Load the CIFAR10 test dataset and apply the same transformations as for training\n",
        "testset = torchvision.datasets.CIFAR10(root=\"./\", train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Create a DataLoader for test data\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LMb7NPmiNfo"
      },
      "source": [
        "Let's view some of the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01HjHP7EiQDq"
      },
      "outputs": [],
      "source": [
        "# Define a function to display images\n",
        "def imshow(img):\n",
        "    # Unnormalize the image\n",
        "    # The normalization was done using this formula:\n",
        "    # img_norm = (img - mean) / std\n",
        "    # So we unnormalize as follows:\n",
        "    img = img * 0.5 + 0.5\n",
        "\n",
        "    npimg = img.numpy()\n",
        "\n",
        "    #The image array is typically stored in the order [height, width, channels],\n",
        "    #but the plt.imshow function expects the image array to be stored in the\n",
        "    #order [channels, width, height].\n",
        "    #The np.transpose function transposes the image array,\n",
        "    #so that it is stored in the correct order.\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "    # Display the image\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show the images using the imshow function\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "# Print labels\n",
        "print('Labels:', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l62CkyIwtSOv"
      },
      "source": [
        "### **TODO 11:**\n",
        "\n",
        "Implement a simple CNN model with the architecure defined in the comments to classify images from the CIFAR-10 dataset. The CIFAR-10 dataset comprises 3x32x32 pixel RGB color images with 10 distinct categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fL3F-7Rntog"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Define the layers of the neural network architecture\n",
        "\n",
        "        # First convolutional layer: 3 input channels, 6 output channels, kernel size 5x5\n",
        "\n",
        "\n",
        "        # Max pooling layer with kernel size 2x2 and stride 2\n",
        "\n",
        "\n",
        "        # Second convolutional layer: 6 input channels, 16 output channels, kernel size 5x5\n",
        "\n",
        "\n",
        "        # First fully connected layer: 16*5*5 input features, 120 output features\n",
        "\n",
        "\n",
        "        # Second fully connected layer: 120 input features, 84 output features\n",
        "\n",
        "\n",
        "        # Third fully connected layer: 84 input features, 10 output features (for classification)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Apply first convolutional layer, followed by ReLU activation and max pooling\n",
        "\n",
        "\n",
        "        # Apply second convolutional layer, followed by ReLU activation and max pooling\n",
        "\n",
        "\n",
        "        # The output from the convolutional and pooling layers is in the form of a\n",
        "        # 3D tensor (height, width, depth or channels).\n",
        "        # To feed this tensor into a fully connected layer,\n",
        "        # it needs to be flattened into a 1D tensor.\n",
        "        # Reshape tensor for fully connected layers\n",
        "        # A2D tensor with a shape of [batch_size, 16 * 5 * 5].\n",
        "\n",
        "\n",
        "        # Apply first fully connected layer, followed by ReLU activation\n",
        "\n",
        "\n",
        "        # Apply second fully connected layer, followed by ReLU activation\n",
        "\n",
        "\n",
        "        # Apply third fully connected layer (output layer)\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create an instance of the neural network and move it to the specified device (e.g., GPU)\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nijieuxptag6"
      },
      "source": [
        "### **TODO 12:**\n",
        "\n",
        "Implement the training loop for your CNN model. The loop should feed input data to the model, compute the loss using the correct labels, and update the model weights using an optimization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzK6ohj5oNCT"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, dataloader: DataLoader, epoch_num: int):\n",
        "    # Define the loss criterion and optimizer\n",
        "\n",
        "    for epoch in range(epoch_num):  # Loop over the dataset for multiple epochs\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Iterate over mini-batches of data\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            # Get the inputs; data is a list of [inputs, labels]\n",
        "\n",
        "            # Move inputs to the specified device\n",
        "\n",
        "            # Move labels to the specified device\n",
        "\n",
        "            # Zero the gradients in the optimizer\n",
        "\n",
        "            # Forward pass through the model\n",
        "\n",
        "            # Calculate the loss\n",
        "\n",
        "            # Compute gradients\n",
        "\n",
        "            # Update model parameters\n",
        "\n",
        "            # Print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:  # Print every 2000 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')  # Training loop is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANBYPtAMP2jT"
      },
      "source": [
        "Train this CNN on the training dataset with 3 epoch (this may take a few moments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HixhBHaqtmZU"
      },
      "outputs": [],
      "source": [
        "train(net, trainloader, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJggxnCVuRxU"
      },
      "source": [
        "Now that the CNN has been trained, let's test it on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifYWr5qFrzdc"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
        "\n",
        "net.cuda()\n",
        "images = images.cuda()\n",
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW3U7pourwjT"
      },
      "source": [
        "Let us look at how the network performs on the whole dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y27_n-djuEdz"
      },
      "outputs": [],
      "source": [
        "# A function to evaluate the performance of a given neural network model\n",
        "# using a test dataset.\n",
        "# It calculates the accuracy of the model's predictions on the test data.\n",
        "def calculate_accuracy(model: nn.Module, dataloader: DataLoader, max_samples=None) -> float:\n",
        "    correct_predictions = 0  # Initialize the count of correctly predicted samples\n",
        "    total_samples = 0    # Initialize the count of total samples\n",
        "    inference_count = 0  # Initialize the count of inferences made\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for batch_data in dataloader:\n",
        "            images, labels = batch_data  # Separate images and labels from the batch\n",
        "\n",
        "            images = images.to(device)  # Move images to the specified device\n",
        "            labels = labels.to(device)  # Move labels to the specified device\n",
        "\n",
        "            outputs = model(images)  # Forward pass to get model predictions\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get predicted class indices\n",
        "\n",
        "            total_samples += labels.size(0)  # Increment the total count of samples\n",
        "            correct_predictions += (predicted == labels).sum().item()  # Count correct predictions\n",
        "\n",
        "            if max_samples:  # Check if a maximum number of samples for testing is specified\n",
        "                inference_count += images.shape[0]  # Increment the count of inferences made\n",
        "                if inference_count > max_samples:  # Stop testing if maximum samples reached\n",
        "                    break\n",
        "\n",
        "    accuracy = 100 * correct_predictions / total_samples  # Calculate the accuracy as a percentage\n",
        "    return accuracy\n",
        "\n",
        "score = calculate_accuracy(net, testloader)\n",
        "print('Accuracy of the network on the test images: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2_4MK0B1f1z"
      },
      "source": [
        "### **TODO 13:**\n",
        "1. In the CNN model we have created, how many layers have trainable weights? Please list all the dimensions of each layer's weights.\n",
        "\n",
        "2. For each of the layers with traiable weights, please list their input and output dimension.\n",
        "\n",
        "3. The outputs of a neural network before the activation function is applied are named as \"logits\". For multi-class clasification, we know that we need to apply softmax activation function to logits and then compute cross-entropy loss on it. However, in our `Net` class where we define the model architecture, we haven't explicitly initialized a softmax function or applied it to the model's final output within the forward function. Can you explain why by referring to PyTorch's [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) documentation?\n",
        "\n",
        "4. Why is cross-entropy loss prefered over MSE loss for multi-class classification?\n",
        "\n",
        "5. Suppose we have a vector of logits `Z`, with a dimension of `M`. This dimension corresponds to the number of output classes in a classification task, such as the Cifar10 classification task where `M` = 10. First, we apply the softmax function to the logits `Z`, which yields a vector of predicted probabilities for each class, denoted as `P`. Then, we compute the cross-entropy loss, denoted as `L`, using the probabilities vector `P` and the true class labels `Y`, which are provided in the form of a one-hot encoded vector of the same length `M`. Derive the gradient of the loss `L` with respect to the logits `Z`, namely `dL/dZ`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtfggcxyP2jU"
      },
      "source": [
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdR8TYHPP2jU"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xqQLUTPfi0JZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}